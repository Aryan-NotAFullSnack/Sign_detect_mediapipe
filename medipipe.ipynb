{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2b46259-9e21-4413-a1b6-443674dab415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: mediapipe in /opt/anaconda3/lib/python3.12/site-packages (0.10.15)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (0.4.34)\n",
      "Requirement already satisfied: jaxlib in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (0.4.34)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (3.8.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (4.25.5)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /opt/anaconda3/lib/python3.12/site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (0.5.0)\n",
      "Requirement already satisfied: opt-einsum in /opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /opt/anaconda3/lib/python3.12/site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python mediapipe numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "101a95f0-3fff-4f52-b388-a704548b69c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729680853.005982 2557746 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1729680853.032817 2557931 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729680853.051712 2557931 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit the webcam window.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1729680854.877696 2557932 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "2024-10-23 16:24:14.941 python[80972:2557746] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-10-23 16:24:14.941 python[80972:2557746] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to grab frame\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe hands and drawing utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize the Hands model\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# Function to recognize simple gestures based on landmarks\n",
    "def recognize_simple_gesture(landmarks):\n",
    "    thumb_tip = landmarks[4]   # Thumb tip\n",
    "    index_tip = landmarks[8]   # Index finger tip\n",
    "    middle_tip = landmarks[12] # Middle finger tip\n",
    "\n",
    "    if index_tip[1] < landmarks[6][1] and middle_tip[1] < landmarks[10][1]:\n",
    "        return 'Peace'\n",
    "    elif index_tip[1] < landmarks[6][1]:\n",
    "        return 'One'\n",
    "    else:\n",
    "        return 'Fist'\n",
    "\n",
    "# Open the webcam and process frames\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "else:\n",
    "    print(\"Press 'q' to quit the webcam window.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    \n",
    "    # Flip the frame horizontally for a mirror-like view\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert the frame to RGB for MediaPipe processing\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    # Default gesture if no hand is detected\n",
    "    gesture = 'No Hand Detected'\n",
    "\n",
    "    # If hand landmarks are detected, draw them and recognize the gesture\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Extract landmark coordinates\n",
    "            landmarks = []\n",
    "            h, w, _ = frame.shape\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.append([int(lm.x * w), int(lm.y * h)])\n",
    "            \n",
    "            # Recognize gesture using the landmarks\n",
    "            gesture = recognize_simple_gesture(landmarks)\n",
    "\n",
    "    # Display the recognized gesture on the frame\n",
    "    cv2.putText(frame, f'Gesture: {gesture}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow('Sign Language Recognition', frame)\n",
    "\n",
    "    # Exit the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8532057e-f366-4cda-aa53-2ac676b62310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
